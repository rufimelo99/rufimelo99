<!DOCTYPE html>
<html lang="en">
<!-- Template: https://github.com/luost26/academic-homepage -->
<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <title>Publications - Rui Melo</title>

    <!-- Stylesheets -->
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/twitter-bootstrap/4.6.0/css/bootstrap.min.css" integrity="sha512-P5MgMn1jBN01asBgU0z60Qk4QxiXo86+wlFahKrsQf37c9cro517WzVSPPV1tDKzhku2iJ2FVgL67wG03SGnNA==" crossorigin="anonymous" />
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.5.1/css/all.min.css" integrity="sha512-DTOQO9RWCH3ppGqcWaEA1BIZOC6xxalwEsw9c2QQeAIftl+Vegovlnee1c9QX4TctnWMn13TZye+giMm8e2LwA==" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/academicons/1.9.1/css/academicons.min.css" integrity="sha512-b1ASx0WHgVFL5ZQhTgiPWX+68KjS38Jk87jg7pe+qC7q9YkEtFq0z7xCglv7qGIs/68d3mAp+StfC8WKC5SSAg==" crossorigin="anonymous" />
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Lato:ital,wght@0,300;0,400;0,700;0,900;1,300;1,400;1,700;1,900&family=Raleway:ital,wght@0,300;0,400;0,500;1,300;1,400;1,500&display=swap" rel="stylesheet">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.11/dist/katex.min.css" integrity="sha384-nB0miv6/jRmo5UMMR1wu3Gz6NLsoTkbqJghGIsx//Rlm+ZU03BU6SQNC66uf4l5+" crossorigin="anonymous">
    <link rel="stylesheet" href="/academic-homepage/assets/css/global.css">
</head>
<body class="bg-light" data-spy="scroll" data-target="#navbar-year" data-offset="100">
    <nav class="navbar navbar-expand-sm navbar-light bg-light fixed-top mb-5 shadow-sm">
    <div class="container-lg">
        <a class="navbar-brand"><strong>Rui Melo</strong></a>
        <button class="navbar-toggler" style="font-size: 1em; padding: 0.5em;" type="button" data-toggle="collapse" data-target="#navbarResponsive" aria-controls="navbarResponsive" aria-expanded="false" aria-label="Toggle navigation">
            <i class="fas fa-map"></i> Menu
        </button>

        <div class="collapse navbar-collapse" id="navbarResponsive">
            <ul class="navbar-nav ml-auto">
                
                <li class="nav-item ">
                    <a class="nav-link" href="/academic-homepage/">Home</a>
                </li>
                
                <li class="nav-item active">
                    <a class="nav-link" href="/academic-homepage/publications">Publications</a>
                </li>
                
                <li class="nav-item ">
                    <a class="nav-link" href="/academic-homepage/showcase">Showcase</a>
                </li>
                
            </ul>
        </div>
    </div>
</nav>

    <div class="container-lg">
        

<div class="row">
    <div class="col-12 col-lg-10">
        
        
        <h2 class="pt-4" id="year-2025">2025</h2>
        <div class="my-0 p-0 bg-white shadow-sm rounded-xl">
            
                
<div class="d-none d-md-block">
    <div class="row no-gutters  border-gray">
        <div class="col-md-3 col-xl-2 mb-md-0 p-md-3"><img data-src="/academic-homepage/assets/images/covers/arxiv.png" alt="Are Sparse Autoencoders Useful for Java Function Bug Detection?" class="lazy w-100 rounded-sm" src="/academic-homepage/assets/images/empty_300x200.png"></div>
        <div class="col-md-9 col-xl-10 p-3 pl-md-0">
            <h5 class="mt-0 mb-1 font-weight-normal">Are Sparse Autoencoders Useful for Java Function Bug Detection?</h5>
            <p class="mt-0 mb-0 small"><span class="text-body">
            <strong>Rui Melo</strong>, </span><span class="text-body">
            Claudia Mamede, </span><span class="text-body">
            Andre Catarino, </span><span class="text-body">
            Rui Abreu, </span><span class="text-body">
            Henrique Lopes Cardoso</span></p>
            <p class="mt-0 mb-0 small"><i>arXiv preprint arXiv:2505.10375</i> 2025 </p>
            <p class="mt-0 mb-0 small text-muted">Software vulnerabilities such as buffer overflows and SQL injections are a major source of security breaches. Traditional methods for vulnerability detection remain essential but are limited by high false positive rates, scalability issues, and reliance on manual effort. These constraints have driven interest in AI-based approaches to automated vulnerability detection and secure code generation. While Large Language Models (LLMs) have opened new avenues for classification tasks, their complexity and opacity pose challenges for interpretability and deployment. Sparse Autoencoders offer a promising solution to this problem. We explore whether SAEs can serve as a lightweight, interpretable alternative for bug detection in Java functions. We evaluate the effectiveness of SAEs when applied to representations from GPT-2 Small and Gemma 2B, examining their capacity to highlight buggy behaviour without fine-tuning the underlying LLMs. We found that SAE-derived features enable bug detection with an F1 score of up to 89%, consistently outperforming fine-tuned transformer encoder baselines. Our work provides the first empirical evidence that SAEs can be used to detect software bugs directly from the internal representations of pretrained LLMs, without any fine-tuning or task-specific supervision.</p>
            
            <p class="small pb-0 mb-0 lh-125 text-muted abstract-links">
                
            </p>

        </div>
    </div>
</div>

<div class="row no-gutters d-md-none  border-gray rounded-xl-top rounded-xl-bottom lazy" data-src="/academic-homepage/assets/images/covers/arxiv.png">
    <div class="w-100 rounded-xl-top rounded-xl-bottom" style="background-color: rgba(255,255,255,0.9);">
        <div class="d-flex align-items-start flex-column py-3 px-4">
            <div class="mb-auto"></div>
            <div>
                <h5 class="mt-0 mb-1 font-weight-normal">Are Sparse Autoencoders Useful for Java Function Bug Detection?</h5>
                <p class="mt-0 mb-0 small"><span class="text-body">
            <strong>Rui Melo</strong>, </span><span class="text-body">
            Claudia Mamede, </span><span class="text-body">
            Andre Catarino, </span><span class="text-body">
            Rui Abreu, </span><span class="text-body">
            Henrique Lopes Cardoso</span></p>
                <p class="mt-0 mb-0 small"><i>arXiv preprint arXiv:2505.10375</i> 2025 </p>
                <p class="mt-0 mb-0 small text-muted">Software vulnerabilities such as buffer overflows and SQL injections are a major source of security breaches. Traditional methods for vulnerability detection remain essential but are limited by high false positive rates, scalability issues, and reliance on manual effort. These constraints have driven interest in AI-based approaches to automated vulnerability detection and secure code generation. While Large Language Models (LLMs) have opened new avenues for classification tasks, their complexity and opacity pose challenges for interpretability and deployment. Sparse Autoencoders offer a promising solution to this problem. We explore whether SAEs can serve as a lightweight, interpretable alternative for bug detection in Java functions. We evaluate the effectiveness of SAEs when applied to representations from GPT-2 Small and Gemma 2B, examining their capacity to highlight buggy behaviour without fine-tuning the underlying LLMs. We found that SAE-derived features enable bug detection with an F1 score of up to 89%, consistently outperforming fine-tuned transformer encoder baselines. Our work provides the first empirical evidence that SAEs can be used to detect software bugs directly from the internal representations of pretrained LLMs, without any fine-tuning or task-specific supervision.</p>
                
                <p class="small pb-0 mb-0 lh-125 text-muted abstract-links">
                    
                </p>
            </div>
        </div>
    </div>

</div>
            
        </div>
        
        
        <h2 class="pt-4" id="year-2024">2024</h2>
        <div class="my-0 p-0 bg-white shadow-sm rounded-xl">
            
                
<div class="d-none d-md-block">
    <div class="row no-gutters border-bottom border-gray">
        <div class="col-md-3 col-xl-2 mb-md-0 p-md-3"><img data-src="/academic-homepage/assets/images/covers/NIPS.png" alt="SaulLM-54B & SaulLM-141B: Scaling Up Domain Adaptation for the Legal Domain" class="lazy w-100 rounded-sm" src="/academic-homepage/assets/images/empty_300x200.png"></div>
        <div class="col-md-9 col-xl-10 p-3 pl-md-0">
            <h5 class="mt-0 mb-1 font-weight-normal">SaulLM-54B & SaulLM-141B: Scaling Up Domain Adaptation for the Legal Domain</h5>
            <p class="mt-0 mb-0 small"><span class="text-body">
            Pierre Colombo, </span><span class="text-body">
            Telmo Pires, </span><span class="text-body">
            Malik Boudiaf, </span><span class="text-body">
            <strong>Rui Melo</strong>, </span><span class="text-body">
            Dominic Culver, </span><span class="text-body">
            Sofia Morgado, </span><span class="text-body">
            Etienne Malaboeuf, </span><span class="text-body">
            Gabriel Hautreux, </span><span class="text-body">
            Johanne Charpentier, </span><span class="text-body">
            Michael Desa</span></p>
            <p class="mt-0 mb-0 small"><i>NeurIPS</i> 2024 </p>
            <p class="mt-0 mb-0 small text-muted">In this paper, we introduce SaulLM-54B and SaulLM-141B, two large language models (LLMs) tailored for the legal sector. These models, which feature architectures of 54 billion and 141 billion parameters, respectively, are based on the Mixtral architecture. The development of SaulLM-54B and SaulLM-141B is guided by large-scale domain adaptation, divided into three strategies: (1) the exploitation of continued pretraining involving a base corpus that includes over 540 billion of legal tokens, (2) the implementation of a specialized legal instruction-following protocol, and (3) the alignment of model outputs with human preferences in legal interpretations. The integration of synthetically generated data in the second and third steps enhances the models' capabilities in interpreting and processing legal texts, effectively reaching state-of-the-art performance and outperforming previous open-source models on LegalBench-Instruct. This work explores the trade-offs involved in domain-specific adaptation at this scale, offering insights that may inform future studies on domain adaptation using strong decoder models. Building upon SaulLM-7B, this study refines the approach to produce an LLM better equipped for legal tasks. We are releasing base, instruct, and aligned versions on top of SaulLM-54B and SaulLM-141B under the MIT License to facilitate reuse and collaborative research.</p>
            
            <p class="small pb-0 mb-0 lh-125 text-muted abstract-links">
                
            </p>

        </div>
    </div>
</div>

<div class="row no-gutters d-md-none border-bottom border-gray rounded-xl-top  lazy" data-src="/academic-homepage/assets/images/covers/NIPS.png">
    <div class="w-100 rounded-xl-top " style="background-color: rgba(255,255,255,0.9);">
        <div class="d-flex align-items-start flex-column py-3 px-4">
            <div class="mb-auto"></div>
            <div>
                <h5 class="mt-0 mb-1 font-weight-normal">SaulLM-54B & SaulLM-141B: Scaling Up Domain Adaptation for the Legal Domain</h5>
                <p class="mt-0 mb-0 small"><span class="text-body">
            Pierre Colombo, </span><span class="text-body">
            Telmo Pires, </span><span class="text-body">
            Malik Boudiaf, </span><span class="text-body">
            <strong>Rui Melo</strong>, </span><span class="text-body">
            Dominic Culver, </span><span class="text-body">
            Sofia Morgado, </span><span class="text-body">
            Etienne Malaboeuf, </span><span class="text-body">
            Gabriel Hautreux, </span><span class="text-body">
            Johanne Charpentier, </span><span class="text-body">
            Michael Desa</span></p>
                <p class="mt-0 mb-0 small"><i>NeurIPS</i> 2024 </p>
                <p class="mt-0 mb-0 small text-muted">In this paper, we introduce SaulLM-54B and SaulLM-141B, two large language models (LLMs) tailored for the legal sector. These models, which feature architectures of 54 billion and 141 billion parameters, respectively, are based on the Mixtral architecture. The development of SaulLM-54B and SaulLM-141B is guided by large-scale domain adaptation, divided into three strategies: (1) the exploitation of continued pretraining involving a base corpus that includes over 540 billion of legal tokens, (2) the implementation of a specialized legal instruction-following protocol, and (3) the alignment of model outputs with human preferences in legal interpretations. The integration of synthetically generated data in the second and third steps enhances the models' capabilities in interpreting and processing legal texts, effectively reaching state-of-the-art performance and outperforming previous open-source models on LegalBench-Instruct. This work explores the trade-offs involved in domain-specific adaptation at this scale, offering insights that may inform future studies on domain adaptation using strong decoder models. Building upon SaulLM-7B, this study refines the approach to produce an LLM better equipped for legal tasks. We are releasing base, instruct, and aligned versions on top of SaulLM-54B and SaulLM-141B under the MIT License to facilitate reuse and collaborative research.</p>
                
                <p class="small pb-0 mb-0 lh-125 text-muted abstract-links">
                    
                </p>
            </div>
        </div>
    </div>

</div>
            
                
<div class="d-none d-md-block">
    <div class="row no-gutters  border-gray">
        <div class="col-md-3 col-xl-2 mb-md-0 p-md-3"><img data-src="/academic-homepage/assets/images/covers/arxiv.png" alt="SaulLM-7B: A pioneering Large Language Model for Law" class="lazy w-100 rounded-sm" src="/academic-homepage/assets/images/empty_300x200.png"></div>
        <div class="col-md-9 col-xl-10 p-3 pl-md-0">
            <h5 class="mt-0 mb-1 font-weight-normal">SaulLM-7B: A pioneering Large Language Model for Law</h5>
            <p class="mt-0 mb-0 small"><span class="text-body">
            Pierre Colombo, </span><span class="text-body">
            Telmo Pessoa Pires, </span><span class="text-body">
            Malik Boudiaf, </span><span class="text-body">
            Dominic Culver, </span><span class="text-body">
            <strong>Rui Melo</strong>, </span><span class="text-body">
            Caio Corro, </span><span class="text-body">
            Andre F. T. Martins, </span><span class="text-body">
            Fabrizio Esposito, </span><span class="text-body">
            Vera Lúcia Raposo, </span><span class="text-body">
            Sofia Morgado, </span><span class="text-body">
            Michael Desa</span></p>
            <p class="mt-0 mb-0 small"><i>arXiv</i> 2024 </p>
            <p class="mt-0 mb-0 small text-muted">In this paper, we introduce SaulLM-7B, a large language model (LLM) tailored for the legal domain. With 7 billion parameters, SaulLM-7B is the first LLM designed explicitly for legal text comprehension and generation. Leveraging the Mistral 7B architecture as its foundation, SaulLM-7B is trained on an English legal corpus of over 30 billion tokens. SaulLM-7B exhibits state-of-the-art proficiency in understanding and processing legal documents. Additionally, we present a novel instructional fine-tuning method that leverages legal datasets to further enhance SaulLM-7B's performance in legal tasks. SaulLM-7B is released under the MIT License.</p>
            
            <p class="small pb-0 mb-0 lh-125 text-muted abstract-links">
                
            </p>

        </div>
    </div>
</div>

<div class="row no-gutters d-md-none  border-gray  rounded-xl-bottom lazy" data-src="/academic-homepage/assets/images/covers/arxiv.png">
    <div class="w-100  rounded-xl-bottom" style="background-color: rgba(255,255,255,0.9);">
        <div class="d-flex align-items-start flex-column py-3 px-4">
            <div class="mb-auto"></div>
            <div>
                <h5 class="mt-0 mb-1 font-weight-normal">SaulLM-7B: A pioneering Large Language Model for Law</h5>
                <p class="mt-0 mb-0 small"><span class="text-body">
            Pierre Colombo, </span><span class="text-body">
            Telmo Pessoa Pires, </span><span class="text-body">
            Malik Boudiaf, </span><span class="text-body">
            Dominic Culver, </span><span class="text-body">
            <strong>Rui Melo</strong>, </span><span class="text-body">
            Caio Corro, </span><span class="text-body">
            Andre F. T. Martins, </span><span class="text-body">
            Fabrizio Esposito, </span><span class="text-body">
            Vera Lúcia Raposo, </span><span class="text-body">
            Sofia Morgado, </span><span class="text-body">
            Michael Desa</span></p>
                <p class="mt-0 mb-0 small"><i>arXiv</i> 2024 </p>
                <p class="mt-0 mb-0 small text-muted">In this paper, we introduce SaulLM-7B, a large language model (LLM) tailored for the legal domain. With 7 billion parameters, SaulLM-7B is the first LLM designed explicitly for legal text comprehension and generation. Leveraging the Mistral 7B architecture as its foundation, SaulLM-7B is trained on an English legal corpus of over 30 billion tokens. SaulLM-7B exhibits state-of-the-art proficiency in understanding and processing legal documents. Additionally, we present a novel instructional fine-tuning method that leverages legal datasets to further enhance SaulLM-7B's performance in legal tasks. SaulLM-7B is released under the MIT License.</p>
                
                <p class="small pb-0 mb-0 lh-125 text-muted abstract-links">
                    
                </p>
            </div>
        </div>
    </div>

</div>
            
        </div>
        
        
        <h2 class="pt-4" id="year-2023">2023</h2>
        <div class="my-0 p-0 bg-white shadow-sm rounded-xl">
            
                
<div class="d-none d-md-block">
    <div class="row no-gutters  border-gray">
        <div class="col-md-3 col-xl-2 mb-md-0 p-md-3"><img data-src="/academic-homepage/assets/images/covers/EPIA.png" alt="A Semantic Search System for the Supremo Tribunal de Justiça" class="lazy w-100 rounded-sm" src="/academic-homepage/assets/images/empty_300x200.png"></div>
        <div class="col-md-9 col-xl-10 p-3 pl-md-0">
            <h5 class="mt-0 mb-1 font-weight-normal">A Semantic Search System for the Supremo Tribunal de Justiça</h5>
            <p class="mt-0 mb-0 small"><span class="text-body">
            <strong>Rui Melo</strong>, </span><span class="text-body">
            Pedro A. Santos, </span><span class="text-body">
            João Dias</span></p>
            <p class="mt-0 mb-0 small"><i>EPIA Conference on Artificial Intelligence)</i> 2023 </p>
            <p class="mt-0 mb-0 small text-muted">Many information retrieval systems use lexical approaches to retrieve information. Such approaches have multiple limitations, and these constraints are exacerbated when tied to specific domains, such as the legal one. Large language models, such as BERT, deeply understand a language and may overcome the limitations of older methodologies, such as BM25. This work investigated and developed a prototype of a Semantic Search System to assist the Supremo Tribunal de Justiça (Portuguese Supreme Court of Justice) in its decision-making process. We built a Semantic Search System that uses specially trained BERT models (Legal-BERTimbau variants) and a Hybrid Search System that incorporates both lexical and semantic techniques by combining the capabilities of BM25 and the potential of Legal-BERTimbau. In this context, we obtained a increase on the discovery metric when compared to BM25 for the first query result. This work also provides information on the most relevant techniques for training a Large Language Model adapted to Portuguese jurisprudence and introduces a new technique of Metadata Knowledge Distillation.</p>
            
            <p class="small pb-0 mb-0 lh-125 text-muted abstract-links">
                
            </p>

        </div>
    </div>
</div>

<div class="row no-gutters d-md-none  border-gray rounded-xl-top rounded-xl-bottom lazy" data-src="/academic-homepage/assets/images/covers/EPIA.png">
    <div class="w-100 rounded-xl-top rounded-xl-bottom" style="background-color: rgba(255,255,255,0.9);">
        <div class="d-flex align-items-start flex-column py-3 px-4">
            <div class="mb-auto"></div>
            <div>
                <h5 class="mt-0 mb-1 font-weight-normal">A Semantic Search System for the Supremo Tribunal de Justiça</h5>
                <p class="mt-0 mb-0 small"><span class="text-body">
            <strong>Rui Melo</strong>, </span><span class="text-body">
            Pedro A. Santos, </span><span class="text-body">
            João Dias</span></p>
                <p class="mt-0 mb-0 small"><i>EPIA Conference on Artificial Intelligence)</i> 2023 </p>
                <p class="mt-0 mb-0 small text-muted">Many information retrieval systems use lexical approaches to retrieve information. Such approaches have multiple limitations, and these constraints are exacerbated when tied to specific domains, such as the legal one. Large language models, such as BERT, deeply understand a language and may overcome the limitations of older methodologies, such as BM25. This work investigated and developed a prototype of a Semantic Search System to assist the Supremo Tribunal de Justiça (Portuguese Supreme Court of Justice) in its decision-making process. We built a Semantic Search System that uses specially trained BERT models (Legal-BERTimbau variants) and a Hybrid Search System that incorporates both lexical and semantic techniques by combining the capabilities of BM25 and the potential of Legal-BERTimbau. In this context, we obtained a increase on the discovery metric when compared to BM25 for the first query result. This work also provides information on the most relevant techniques for training a Large Language Model adapted to Portuguese jurisprudence and introduces a new technique of Metadata Knowledge Distillation.</p>
                
                <p class="small pb-0 mb-0 lh-125 text-muted abstract-links">
                    
                </p>
            </div>
        </div>
    </div>

</div>
            
        </div>
        
    </div>

    <div class="col-2 d-none d-lg-block">
        <div id="navbar-year" class="nav nav-pills flex-column sticky-top" style="top: 80px">
            
            <a class="nav-link d-block" href="#year-2025">2025</a>
            
            <a class="nav-link d-block" href="#year-2024">2024</a>
            
            <a class="nav-link d-block" href="#year-2023">2023</a>
            
        </div>
    </div>

</div>

    </div>
    <footer class="footer">
    <div class="container-lg">
        <div class="row my-3">
            <div class="col-6">
                <div class="text-muted">
                    <i>Last updated: Jul 2025</i>
                </div>
            </div>
            <div class="col-6">
                <div class="text-right text-muted">
                    
                </div>
            </div>
        </div>
    </div>
</footer>


    <!-- Scripts -->
    <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
    <script src="//cdnjs.cloudflare.com/ajax/libs/jquery.lazy/1.7.9/jquery.lazy.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.7/umd/popper.min.js" integrity="sha384-UO2eT0CpHqdSJQ6hJty5KVphtPhzWj9WO1clHTMGa3JDZwrnQq4sF86dIHNDz0W1" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/twitter-bootstrap/4.6.0/js/bootstrap.min.js" integrity="sha512-XKa9Hemdy1Ui3KSGgJdgMyYlUg1gM+QhL6cnlyTe2qzMCYm4nAZ1PsVerQzTTXzonUR+dmswHqgJPuwCq1MaAg==" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/github-buttons/2.14.2/buttons.min.js" integrity="sha512-OYwZx04hKFeFNYrWxIyo3atgGpb+cxU0ENWBZs72X7T9U+NoHPM1ftUn/Mfw7dRDXrqWA6M1wBg6z6fGE32aeA==" crossorigin="anonymous"></script>
    <script src="https://unpkg.com/masonry-layout@4/dist/masonry.pkgd.min.js"></script>
    <script src="https://unpkg.com/imagesloaded@5/imagesloaded.pkgd.min.js"></script>
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.11/dist/katex.min.js" integrity="sha384-7zkQWkzuo3B5mTepMUcHkMB5jZaolc2xDwL6VFqjFALcbeS9Ggm/Yr2r3Dy4lfFg" crossorigin="anonymous"></script>
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.11/dist/contrib/auto-render.min.js" integrity="sha384-43gviWU0YVjaDtb/GhzOouOXtZMP/7XUzwPTstBeZFe/+rCMvRwr4yROQP43s0Xk" crossorigin="anonymous"></script>
    <script>
        document.addEventListener("DOMContentLoaded", function() {
            renderMathInElement(document.body, {
              delimiters: [
                  {left: '$$', right: '$$', display: true},
                  {left: '$', right: '$', display: false}
              ],
              throwOnError : false
            });
        });
    </script>
    <script src="/academic-homepage/assets/js/common.js"></script>
    <script src="/academic-homepage/assets/js/bubble_visual_hash.js"></script>
</body>
</html>
